---

###
#  Resources
###

resources:

  - name: events
    type: content-events
    source:
      api_root: http://content-events-api:5000
      status: queued

  - name: bridge
    type: docker-image
    source:
      repository: openstax/rap-spike-bridge
      tag: latest

  # remember to use `fly ... -l credentials.yml` for these parameters
  - name: s3
    type: s3-simple
    source:
      access_key_id: ((aws-access-key))
      secret_access_key: ((aws-secret-key))
      bucket: ((s3bucket))
      region: ((s3region))
      options:
        - --exclude='*.metadata.json'

###
#  Resource Types
###

resource_types:

  - name: content-events
    type: docker-image
    source:
      repository: openstax/content-event-resource
      tag: 0.1.0

  - name: s3-simple
    type: docker-image
    source:
      repository: 18fgsa/s3-resource-simple

###
#  Pipeline Jobs
###

jobs:

  - name: pulling-content-into-s3
    plan:

      - get: events
        trigger: true
        version: every

      - get: bridge

      - task: extraction
        image: bridge
        file: bridge/rootfs/tasks/extract-content.yml
        vars:
          db_url: "postgresql://rhaptos@cnx-db/repository"
        input_mapping:
          identifier-struct: events
        output_mapping:
          content: content

      - task: separate contents and resources
        config:
          platform: linux
          image_resource:
            type: docker-image
            source:
              repository: alpine
          inputs:
            - name: content
            - name: events
          outputs:
            - name: contents
            - name: resources
          run:
            path: sh
            args:
              - -c
              - |
                ls -lah content
                mv content/archive-response.raw.json contents/$(cat events/ident_hash).json
                mv content/* resources/
                wget -O "contents/$(cat events/ident_hash).html" "https://archive-staging.cnx.org/contents/$(cat events/ident_hash).html"

      - put: s3
        inputs:
          - contents
          - resources

      - task: fix up resource content-types -_-
        config:
          platform: linux
          image_resource:
            type: docker-image
            source:
              repository: 18fgsa/s3-resource-simple
          inputs:
            - name: resources
          run:
            path: sh
            args:
              - -c
              - |
                for meta in resources/*.metadata.json
                do
                  content_type="$(python3 -c "import json; print(json.load(open('$meta'))['media_type'])")"
                  filename=$(basename ${meta/.metadata.json/})
                  aws s3api copy-object --bucket ((s3bucket)) --copy-source /((s3bucket))/resources/$filename --key resources/$filename --metadata-directive REPLACE --content-disposition=inline --content-type=$content_type --metadata "$(cat $meta)"
                done
          params:
            AWS_ACCESS_KEY_ID: ((aws-access-key))
            AWS_SECRET_ACCESS_KEY: ((aws-secret-key))
            AWS_DEFAULT_REGION: ((s3region))
